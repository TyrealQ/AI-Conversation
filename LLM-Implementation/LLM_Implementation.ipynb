{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Ka43lJ_KAU8_nsLVZTFrBKWCWnUHLhIj",
      "authorship_tag": "ABX9TyNVhOce6GLt3cv81Oej5qsX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TyrealQ/AI-Conversation/blob/main/LLM_Implementation/LLM_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Overview\n",
        "\n",
        "This comprehensive guide provides code and detailed instructions for implementing OpenAI's GPT models across applications. Each section delivers clear explanations alongside practical code snippets, enabling effective LLM integration. The codebook serves both newcomers and experienced practitioners as a reference resource, helping users leverage language models efficiently to enhance analytical capabilities and streamline workflows.\n",
        "\n",
        "Code authored by: **[Tyreal Qian](https://tyrealq.github.io/)**"
      ],
      "metadata": {
        "id": "px0_QeFkUfRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "7u3iTnKFgYDU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q5yTdSOUFjh"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "!pip install datasets\n",
        "!pip install openai\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interacting with the operating system\n",
        "import os\n",
        "\n",
        "# System-specific parameters and functions\n",
        "import sys\n",
        "\n",
        "# Handling JSON data\n",
        "import json\n",
        "\n",
        "# Data analysis and manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Handling large datasets efficiently\n",
        "import datasets\n",
        "\n",
        "# Loading datasets from Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Displaying progress bars in loops\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Handling dictionary default values efficiently\n",
        "from collections import defaultdict\n",
        "\n",
        "# Creating interactive widgets\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Display utilities for IPython environments\n",
        "from IPython.display import display, JSON\n",
        "\n",
        "# Interacting with OpenAI models\n",
        "from openai import OpenAI\n",
        "\n",
        "# Retrieving stored user credentials in Google Colab\n",
        "from google.colab import userdata\n",
        "\n",
        "# OpenAI API keys can be obtained at https://platform.openai.com\n",
        "api_key = userdata.get('GPT_KEY')\n",
        "\n",
        "# Initializing OpenAI client with the retrieved API key\n",
        "client = OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "I6LUxYmK-UQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aspect-based sentiment analysis (ABSA) for game day experience\n",
        "\n",
        "This project leverages LLMs as an innovative, off-the-shelf solution for ABSA in sports management. It enables scalable and efficient analysis of fan experiences without the complexities of traditional ABSA methods.\n",
        "\n"
      ],
      "metadata": {
        "id": "LI8FgYJjbtnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code and data are available in the [GitHub Repository](https://github.com/TyrealQ/Experience-is-all-you-need_SMR)."
      ],
      "metadata": {
        "id": "oo6mREq65Du2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TGL content classification with [GPT-4o](https://platform.openai.com/docs/models/gpt-4o)\n",
        "\n",
        "This script automates the classification of YouTube video metadata to determine its relevance to the TMRW Golf League (TGL) using GPT-4o. It processes an input Excel file containing video details (title, description, and author) and assigns a filter rating based on predefined relevance criteria.\n",
        "\n",
        "### Key Features:\n",
        "\n",
        "- Utilize GPT-4o to analyze textual content and assign a rating from 1 (unrelated) to 5 (perfect match). Assign \"MANUAL\" for ambiguous cases requiring further review.\n",
        "- Identify direct and indirect connections to TGL, including affiliated players, influencers, and events.\n",
        "- Automatically assign a perfect match (5) if the video's author is @TGL.\n",
        "\n",
        "### Co-developer: [Philip Kang](https://scholar.google.com/citations?user=VynUSnwAAAAJ&hl=en)"
      ],
      "metadata": {
        "id": "gl6S6PMJbusR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "ds = load_dataset(\"tyrealqian/TGL_content_classification\")\n",
        "\n",
        "# Check the dataset structure\n",
        "print(ds)"
      ],
      "metadata": {
        "id": "tvSA0lPx0_KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(ds['train'].to_pandas())\n",
        "df"
      ],
      "metadata": {
        "id": "1poT8quc9Xkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_document(doc_text):  # Define a function to classify a document based on its text input\n",
        "    try:\n",
        "        # Send document text to GPT-4o for classification\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",  # Use GPT-4o model for classification\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",  # Define system instructions\n",
        "                    \"content\": \"You are an expert classifier.\\\n",
        "                    Your task is to evaluate whether the given document text is directly or contextually related to TMRW Golf League (TGL), an innovative golf league in partnership with the PGA TOUR that fuses advanced technology and live action in prime time.\\\n",
        "                    Content can be considered related if it explicitly mentions TGL or indirectly relates through associated players, influencers (e.g., Good Good Golf), events, or highlights (e.g., player shots, funny moments in TGL events).\\\n",
        "                    Special Rule: If the id_author of the video is @TGL, automatically assign a rating of 5 (perfect match) regardless of the content.\\\n",
        "                    Provide a single rating using the following scale:\\\n",
        "                    1 = no match (completely unrelated to TGL or its ecosystem),\\\n",
        "                    2 = weak match (mentions golf but no clear TGL connection),\\\n",
        "                    3 = moderate match (some connection, but TGL is not a key focus),\\\n",
        "                    4 = strong match (clear TGL connection but not exclusively about TGL),\\\n",
        "                    5 = perfect match (explicitly and primarily about TGL, or id_author equals @TGL),\\\n",
        "                    MANUAL = manual check required (unclear cases or possible indirect relevance).\\\n",
        "                    Output must be in valid JSON format with only the key filter_rating. Example: {\\\"filter_rating\\\": 4}.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",  # Provide the document text as user input\n",
        "                    \"content\": f\"{doc_text}\"\n",
        "                }\n",
        "            ],\n",
        "            temperature=0,  # Ensure deterministic results\n",
        "            max_tokens=2048,  # Allow sufficient token space for response\n",
        "            top_p=1,  # Use deterministic sampling\n",
        "            frequency_penalty=0,  # No penalty for frequent words\n",
        "            presence_penalty=0,  # No encouragement for new topics\n",
        "            response_format={\"type\": \"json_object\"}  # Enforce structured JSON output\n",
        "        )\n",
        "\n",
        "        # Parse and validate JSON response\n",
        "        try:\n",
        "            validation = json.loads(response.choices[0].message.content)  # Convert response to JSON\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Invalid JSON response. Defaulting to MANUAL.\")  # Handle malformed responses\n",
        "            validation = {\"filter_rating\": \"MANUAL\"}  # Assign \"MANUAL\" if parsing fails\n",
        "\n",
        "        # Optional debugging prints\n",
        "        print(\"\\nDocument Preview:\")\n",
        "        print(f\"{doc_text[:200]}...\")  # Display first 200 characters of document text\n",
        "        print(\"\\nValidation Result:\")\n",
        "        print(json.dumps(validation, indent=2))  # Pretty-print classification result\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        return json.dumps(validation)  # Return classification result as JSON string\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError processing document: {e}\")  # Handle unexpected errors\n",
        "        return json.dumps({\"filter_rating\": \"MANUAL\"})  # Default to \"MANUAL\" in case of failure\n",
        "\n",
        "# Read input Excel file\n",
        "input_file = ds  # Path to input file\n",
        "output_file = \"/content/drive/MyDrive/DATA/TGL_classified.xlsx\"  # Path to output file\n",
        "\n",
        "try:\n",
        "    print(f\"Reading input file: {input_file}\")\n",
        "    df = pd.read_excel(input_file)  # Load the Excel file into a pandas DataFrame\n",
        "except Exception as e:\n",
        "    print(f\"Error reading input file: {e}\")\n",
        "    exit(1)  # Exit if file read fails\n",
        "\n",
        "# Classify each row with GPT-4o\n",
        "print(\"\\nStarting document classification...\")\n",
        "validations = []  # Initialize an empty list to store classification results\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Classifying Rows\"):  # Iterate through rows with a progress bar\n",
        "    if row['id_author'] == '@TGL':  # Auto-assign rating 5 if author is @TGL\n",
        "        validations.append(json.dumps({\"filter_rating\": 5}))\n",
        "    else:\n",
        "        doc_text = f\"{row['title']} {row['description']}\"  # Combine title and description\n",
        "        result_json_str = classify_document(doc_text)  # Get classification result\n",
        "        validations.append(result_json_str)  # Store result\n",
        "\n",
        "# Parse the returned JSON and store results\n",
        "parsed_validations = [json.loads(v) for v in validations]  # Convert JSON strings to Python objects\n",
        "df['filter_rating'] = [v.get('filter_rating', 'MANUAL') for v in parsed_validations]  # Extract ratings\n",
        "\n",
        "# Save output to a new Excel file\n",
        "try:\n",
        "    df.to_excel(output_file, index=False)  # Save classified results to a new file\n",
        "    print(f\"\\nClassification complete! Results saved to: {output_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving output file: {e}\")\n",
        "    exit(1)  # Exit if file save fails\n",
        "\n",
        "# Optional: Show summary of ratings\n",
        "print(\"\\nRating Summary:\")\n",
        "print(df['filter_rating'].value_counts(dropna=False))  # Display count of each rating category"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MYiGEPxv1K4",
        "outputId": "1adb6d7f-d701-44a4-85b9-8ad4368f490e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading input file: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['video_link', 'title', 'lang', 'author', 'id_author', 'description', 'link_thumbnail', 'tt', 'views', 'likes', 'comments', 'duration', 'type_video'],\n",
            "        num_rows: 33\n",
            "    })\n",
            "})\n",
            "Error reading input file: Invalid file path or buffer object type: <class 'datasets.dataset_dict.DatasetDict'>\n",
            "\n",
            "Starting document classification...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Classifying Rows:  12%|‚ñà‚ñè        | 4/33 [00:01<00:09,  2.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "TGL: Tiger Woods‚Äô Revolutionary Golf League - Everything You Need to Know! Join us as we dive into TGL (Tomorrow‚Äôs Golf League), the exciting new golf league created by Tiger Woods! üèåÔ∏è‚Äç‚ôÇÔ∏è‚ú® In this vid...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 5\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  15%|‚ñà‚ñå        | 5/33 [00:01<00:10,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "CLUTCH PUTT ‚õ≥Ô∏è Justin Rose wins the hole for the Los Angeles Golf Club ü•∂ | TGL on ESPN Watch as Justin Rose nails a 14-foot putt to give the Los Angeles Golf Club the win on hole 8 over Tiger Woods an...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 5\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  27%|‚ñà‚ñà‚ñã       | 9/33 [00:06<00:21,  1.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "Tiger Woods couldn‚Äôt believe Ludvig √Öberg's 32-foot birdie putt | TGL on ESPN Tiger Woods loves Ludvig √Öberg's 32-foot birdie putt to win Hole 5 for The Bay Golf Club at the inaugural TGL event.\n",
            "\n",
            "‚úîÔ∏è S...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 5\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  30%|‚ñà‚ñà‚ñà       | 10/33 [00:07<00:20,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "ÌÉÄÏù¥Í±∞Ïö∞Ï¶à Îß•Í∏∏Î°úÏù¥Í∞Ä Í≥µÎèôÏ†úÏûëÌïú Ï¥àÎåÄÌòï Ïä§ÌÅ¨Î¶∞Í≥®ÌîÑÎ¶¨Í∑∏ TGLÏù¥ ÏãúÏûëÎêúÎã§ ÌÉÄÏù¥Í±∞Ïö∞Ï¶à Îß•Í∏∏Î°úÏù¥Í∞Ä Í≥µÎèôÏ†úÏûëÌïú Ï¥àÎåÄÌòï Ïä§ÌÅ¨Î¶∞Í≥®ÌîÑÎ¶¨Í∑∏ TGLÏù¥ ÏãúÏûëÎê©ÎãàÎã§, Ïñ∏Ï†ú ÏãúÏûëÎêòÍ≥† Í¥ÄÏ†ÑÌè¨Ïù∏Ìä∏Îäî Î¨¥ÏóáÏùºÍπåÏöî?...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 5\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  33%|‚ñà‚ñà‚ñà‚ñé      | 11/33 [00:08<00:18,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "ÌÉÄÏù¥Í±∞ Ïö∞Ï¶àÏôÄ Î°úÎ¶¨ Îß•Í∏∏Î°úÏù¥Í∞Ä Ïä§ÌÅ¨Î¶∞Í≥®ÌîÑÎ•º?? TGL Ïñ¥ÎñªÍ≤å Î≥¥ÏÖ®ÎÇòÏöî?? #TGL #Ïä§ÌÅ¨Î¶∞Í≥®ÌîÑ #Î°úÎ¶¨Îß•Í∏∏Î°úÏù¥Ïä§ÌÅ¨Î¶∞Í≥®ÌîÑ...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 5\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  36%|‚ñà‚ñà‚ñà‚ñã      | 12/33 [00:08<00:16,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "ÏßÄÍ∏àÍªè ÏïÑÎ¨¥ÎèÑ Í≥µÍ∞úÌïòÏßÄ ÏïäÏïòÎçò TGLÏùò Î™®Îì†Í≤É! ÌÉÄÏù¥Í±∞Ïö∞Ï¶àÏùò ÏÉàÎ°úÏö¥ ÌÅ¥ÎüΩ, Í≥µÏù∏Íµ¨, Ï∫êÎîî, Í∑∏Î¶¨Í≥† ÎÜìÏπòÏßÄ ÎßêÏïÑÏïº Ìï† Ìï´ Ìïú Í≤ΩÍ∏∞ ÏùºÏ†ïÍπåÏßÄ Î™®Îëê Ï†ïÎ¶¨Ìï¥Î¥§ÏäµÎãàÎã§. ÎßàÏπ®ÎÇ¥ Îã§Í∞ÄÏò® TGLÎ¶¨Í∑∏Ïóê ÎßéÏùÄ Í∏∞ÎåÄÍ∞êÏù¥ Î™®ÏïÑÏßÄÍ≥† ÏûàÏäµÎãàÎã§. Ïä§ÌÅ¨Î¶∞ Í≥®ÌîÑÎèÑ ÌÉÄÏù¥Í±∞Ïö∞Ï¶àÍ∞Ä ÌïòÎ©¥ Îã§Î•¥Îã§Îäî Ï†ÄÏùò TGL Ïª®ÌÖêÏ∏†Îì§ÏùÄ Í∑∏ÎèôÏïà ÎßéÏùÄ Í¥ÄÏã¨ÏùÑ Î∞õÏïÑ ÏôîÏ£†. Ï∂úÎ≤îÏùÄ ÏïûÎëî TGLÏóê ÎåÄÌï¥ ÏßÄÍ∏àÍπåÏßÄ Í≥µÍ∞ú ...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 5\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  39%|‚ñà‚ñà‚ñà‚ñâ      | 13/33 [00:09<00:14,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "TGL Stadium Exclusive Technology Tour We got a secret first look at TGL new Sofi Stadium. Thanks to @fullswinggolf for bring us along. \n",
            "\n",
            "Don't miss out! Book a FREE 30-minute golf session at The Back ...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 4\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 14/33 [00:10<00:14,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "Euro Truck Simulator 2 (1.53) New Man TGL 2022 by Gaming ModdinG Delivery to Sweden + DLC's & Mods Euro Truck Simulator 2 (1.53) \n",
            "\n",
            "New Man TGL 2022 by Gaming ModdinG Delivery to Sweden Promods map v2....\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 1\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 15/33 [00:11<00:13,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "Was the Inaugural TGL Match In-Person Worth It? (My Honest Experience) I went to the Inaugural TGL match on my own dime and I wanted to share my experience on whether or not it was worth it. Let me kn...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 5\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 16/33 [00:13<00:20,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "The Good, The Bad, and The Ugly: TGL's Opening Night Check out our merch here: https://store.barstoolsports.com/collections/fore-play\n",
            "\n",
            "Download the Barstool Golf Time App: https://beacons.ai/foreplayp...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 4\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 17/33 [00:14<00:17,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "What Happens if the TGL is a MASSIVE Success? What will happen to the PGA Tour if the TGL is a massive hit? \n",
            "7 Diamonds Clothing ‚ñ∂ https://www.7diamonds.com/\n",
            "Use Code: MSG15\n",
            "\n",
            "SkyTrak Here ‚ñ∂ https://sk...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 4\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 18/33 [00:14<00:14,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "TGLÎ¶¨Í∑∏Ïóê ÎßûÏÑ† Í≥®ÌîÑÏ°¥Ïùò Ïã†Í∑ú Ï†ÑÎûµ! ÏãúÌã∞Í≥®ÌîÑ Í∑∏Î¶¨Í≥† Îã§Í∞ÄÏò§Îäî Í≥®ÌîÑÏ°¥ Ìà¨Ïñ¥Ïóê ÎåÄÌï¥ ÏûêÏÑ∏ÌïòÍ≤å Ï†ïÎ¶¨ÌñàÏäµÎãàÎã§. Í≥®ÌîÑÏ°¥Ïù¥ Ï§ëÍµ≠ÏóêÏÑú ÏÉàÎ°≠Í≤å ÏÑ†Î≥¥Ïù∏ ÏãúÌã∞Í≥®ÌîÑÏóê Ïù¥Ïñ¥, Í≥®ÌîÑ Î≥∏Í≥†Ïû•ÏùÑ ÏßÅÏ†ë Í≥µÎûµÌïòÎäî ÏÉàÎ°úÏö¥ Î¶¨Í∑∏Î•º Ï§ÄÎπÑÏ§ëÏù∏Îç∞Ïöî. TGLÏóê ÎßûÏÑ† Í≥®ÌîÑÏ°¥Ïùò ÏÉàÎ°úÏö¥ Ï†ÑÎûµÏù∏ ÏãúÌã∞Í≥®ÌîÑÏùò Ï≤´Î≤àÏß∏ ÎåÄÌöå ÎÇ¥Ïö©Í≥º ÏÉàÎ°úÏö¥ Î¶¨Í∑∏Îäî Î¨¥ÏóáÏù∏ÏßÄ ÏûêÏÑ∏Ìûà ÏïåÏïÑÎ≥¥Í≤†ÏäµÎãàÎã§.\n",
            "\n",
            "Time Stamp\n",
            "00:0...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 2\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 19/33 [00:15<00:12,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "[TGL]ÌÉÄÏù¥Í±∞Ïö∞Ï¶àXÎ°úÎ¶¨Îß•Í∏∏Î°úÏù¥, Í≤ΩÍ∏∞ 2ÏãúÍ∞Ñ Ïª∑! Ïä§ÌÅ¨Î¶∞+ÌïÑÎìúÏùò ÎÅùÌåêÏôïÏù¥ Ïò®Îã§ Ïù¥Î≤à ÏÜåÏãùÏùÄ Ïö∞Î¶¨Ïùò ‚ÄòÍ≥®ÌîÑ Ìô©Ï†ú‚Äô ÌÉÄÏù¥Í±∞ Ïö∞Ï¶àÏôÄ Î°úÎ¶¨ Îß•Í∏∏Î°úÏù¥Í∞Ä Ï∂úÎ≤îÌïòÎäî Ïä§ÌÅ¨Î¶∞ Í≥®ÌîÑ Î¶¨Í∑∏Ïù∏ TGLÏóê ÎåÄÌïú ÏÜåÏãùÏù∏Îç∞Ïöî. TGLÏùÄ ÏßÄÎÇú 1Ïõî Ï∂úÎ≤î ÏòàÏ†ïÏù¥ÏóàÏúºÎÇò, ÎåÄÌöåÏû• ÏßÄÎ∂ïÏù¥ Î∂ïÍ¥¥ÎêòÎäî ÏÇ¨Í≥†Î°ú Ïù∏Ìï¥ 1ÎÖÑ Ïó∞Í∏∞ÎêòÏóàÏäµÎãàÎã§. \n",
            "\n",
            "Ïó∞Í∏∞Îêú ÎßåÌÅº ÎçîÏö±Îçî TGLÏóê ÎåÄÌïú Í¥ÄÏã¨Ïù¥ Í≥†Ï°∞ÎêòÍ≥† ÏûàÎäîÎç∞Ïöî...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 5\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 20/33 [00:16<00:11,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "[TGLÍ≥®ÌîÑ] ÌÉÄÏù¥Í±∞Ïö∞Ï¶à, Îß•Í∏∏Î°úÏù¥Í∞Ä ÎßåÎì†  PGA Ïä§ÌÅ¨Î¶∞Í≥®ÌîÑ? TGL Í≥®ÌîÑÏóê Í¥ÄÌïú Î™®Îì† Í≤É Íµ¨ÎèÖÍ≥º Ï¢ãÏïÑÏöîÎäî ÏòÅÏÉÅ Ï†úÏûëÏóê Ï†ïÎßê, Ï†ïÎßê ÌÅ∞ ÌûòÏù¥ Îê©ÎãàÎã§!\n",
            "\n",
            "==============================================\n",
            "\n",
            "Ïó¨Îü¨Î∂Ñ, Ïö∞Î¶¨ÏãúÍ∞ÑÏúºÎ°ú 1Ïõî 8Ïùº Ïò§Ï†Ñ 11ÏãúÏóê ÌÉÄÏù¥Í±∞Ïö∞Ï¶àÍ∞Ä ÎßåÎì† Ïä§ÌÅ¨Î¶∞Í≥®ÌîÑ,\n",
            "PGAÌà¨Ïñ¥ ÏÑ†ÏàòÎì§Ïù¥ Î™®Ïó¨ Í≤ΩÍ∏∞ÌïòÎäî TGL ...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 5\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 22/33 [00:17<00:07,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "‚ÄòHello, new world.‚Äô Tiger Woods reacts to best career moments in PGA TOUR Studio 82-time PGA TOUR winner Tiger Woods gets an immersive experience at the recently opened PGA TOUR Studios, a cutting-edg...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 2\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 23/33 [00:18<00:07,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "Í≥®ÌîÑÏ°¥ ÏµúÍ≥†Ïùò ÏúÑÍ∏∞! ÌÉÄÏù¥Í±∞ Ïö∞Ï¶àÍ∞Ä ÎßåÎì† Ïä§ÌÅ¨Î¶∞Í≥®ÌîÑ Î¶¨Í∑∏ TGL Ï∂úÎ≤îÍ≥º ÏïûÏúºÎ°ú Í≥®ÌîÑÏ°¥Ïùò Î∞©Ìñ•ÏùÑ ÏòàÏ∏°Ìï¥Î¥§ÏäµÎãàÎã§. Í≥®ÌîÑÏ°¥ÏóêÏÑú ÎßéÏùÄ ÏãúÍ∞ÑÎèôÏïà Ï§ÄÎπÑÌïú Í≥®ÌîÑÏ°¥ Ìà¨ÎπÑÏ†ÑNXÍ∞Ä Ï∂úÏãúÎêòÏóàÏ£†.\n",
            "ÏÇ¨Ïã§Ï†ÅÏù∏ Í∑∏ÎûòÌîΩÍ≥º UI, Í∑∏Î¶¨Í≥† ÎçîÏö± Î™∞ÏûÖÍ∞ê ÏûàÎäî Í≤ΩÍ∏∞ Ïö¥ÏòÅÍ≥º ÎåÄÌòï ÌÑ∞ÏπòÎ™®ÎãàÌÑ∞Í∞Ä Ïù∏ÏÉÅÏ†ÅÏù¥ÏóàÏßÄÎßå... Ïù¥Îü¨Ìïú Î≥ÄÌôîÎäî Î∞îÎ°ú ÏÉàÎ°úÏö¥ Ïä§ÌÅ¨Î¶∞ Í≥®ÌîÑ Î¶¨Í∑∏Ïù∏ TGLÏóê ÎåÄÎπÑÌïú Í≥®ÌîÑÏ°¥Ïùò Ï†ïÏ±ÖÏù¥ÎùºÍ≥† ...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 5\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 24/33 [00:18<00:06,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "ÌÉÄÏù¥Í±∞Ïö∞Ï¶àÏùò Ïä§ÌÅ¨Î¶∞Í≥®ÌîÑÎ¶¨Í∑∏ TGLÏùò Ïó∞Ïù¥ÏùÄ ÏïÖÏû¨ Î∞úÏÉù! Í≥ºÏó∞ TGLÏùÑ ÎßåÎÇòÎ≥º Ïàò ÏûàÏùÑÍπå? ÌÉÄÏù¥Í±∞Ïö∞Ï¶àÏôÄ Î°úÎ¶¨ Îß•Í∏∏Î°úÏù¥Ïùò TGL Î¶¨Í∑∏ÏôÄ Í≥®ÌîÑÏ°¥ Ìà¨ÎπÑÏ†Ñ NXÍ¥ÄÎ†® ÏòÅÏÉÅÏóê Í¥ÄÏã¨Ïù¥ ÎßéÏäµÎãàÎã§.\n",
            "ÌòÑÏû¨ÍπåÏßÄ ÏßÑÌñâÎêú TGLÏùò Ï£ºÏöî Í≤ΩÍ∏∞Ïö¥ÏòÅ ÎÇ¥Ïó≠Í≥º Í∞ÅÏ¢Ö ÏïÖÏû¨ ÏÜåÏãùÎì§ÏùÑ ÏûêÏÑ∏ÌïòÍ≤å Ï†ïÎ¶¨Ìï¥Î≥¥ÏïòÏäµÎãàÎã§.\n",
            "\n",
            "\n",
            "Time Stamp\n",
            "00:00 Ïù∏Ìä∏Î°ú \n",
            "00:20 ÏÉÅÎãπÌûà ÏàúÏ°∞Î°úÏõ†Îçò TGL Ï∂úÎ≤î\n",
            "...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 5\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 25/33 [00:19<00:05,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "TGLÍ≤ΩÍ∏∞Î∞©ÏãùÍ≥º Ïù¥Ìï¥Ìï† Ïàò ÏóÜÎäî Ïù¥ÏÉÅÌïú Í∑úÏπôÎì§ TGL Í≥®ÌîÑ Î¶¨Í∑∏ Í∞úÎßâÏù¥ ÏñºÎßà Ïïà ÎÇ®ÏïòÏäµÎãàÎã§. Í≥®ÌîÑÎ•º Ï¶êÍ∏∞Îäî ÏôÑÏ†ÑÌûà ÏÉàÎ°úÏö¥ Î∞©ÏãùÏùÑ ÏÑ† Î≥¥Ïù¥Í≤†Îã§Í≥† ÏïºÏã¨Ï∞®Í≤å Ï∂úÎ∞ú ÌïòÎäîÎç∞Ïöî. TGL Î¶¨Í∑∏Í∞Ä ÏùºÎ∞ò Í≥®ÌîÑÏôÄ Îã§Î•∏ Í≤ΩÍ∏∞ Ïö¥ÏòÅ Î∞©ÏãùÍ≥º Ïù¥Ìï¥Ìï† Ïàò ÏóÜÎäî Ïù¥ÏÉÅÌïú Í∑úÏ∞©Îì§Ïóê ÎåÄÌï¥ÏÑú ÏÇ¥Ìé¥ Î≥¥Í≤†ÏäµÎãàÎã§....\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 5\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 26/33 [00:20<00:05,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "Îã§Í∞ÄÏò§Îäî TGLÍ≥º ÌíÄ Ïä§Ïúô Ïä§ÌÅ¨Î¶∞Í≥®ÌîÑ! Í≥®ÌîÑÏ°¥Ïù¥ Ï§ÄÎπÑÌïú ÌöåÏã¨Ïùò Ïó≠Ïûë! ÌïòÏù¥Î∏åÎ¶¨Îìú Í≥®ÌîÑ Î¶¨Í∑∏Ïóê ÎåÄÌï¥ ÏòàÏÉÅÌï¥Î≥¥ÏïòÏäµÎãàÎã§. ÏßÄÎÇú ÏòÅÏÉÅÏóêÏÑú TGLÍ≥º Í≥®ÌîÑÏ°¥ Ìà¨ÎπÑÏ†Ñ NXÏóê ÎåÄÌïú Í¥ÄÏã¨Ïù¥ Îú®Í±∞Ïõ†ÏäµÎãàÎã§.\n",
            "Ïò§ÎäòÏùÄ ÏßÄÎÇúÎ≤à ÏòÅÏÉÅÏóêÏÑú Îã§Î£®ÏßÄ ÏïäÏïòÎçò TGLÏùò Í∏∞Ïà†ÏùÑ Í≥®ÌîÑÏ°¥Í≥º ÎπÑÍµêÌïòÍ≥†, ÎåìÍ∏Ä Ï§ë Í∞ÄÏû• Í∂ÅÍ∏àÌï¥ ÌïòÏÖ®Îçò Î∂ÄÎ∂ÑÎì§ÏùÑ Îã§Î§ÑÎ≥¥ÎèÑÎ°ù ÌïòÍ≤†ÏäµÎãàÎã§.\n",
            "\n",
            "* Ìï¥Îãπ ÏòÅÏÉÅ Ï§ë Ï†ÄÏûëÍ∂å Í¥ÄÎ†® ÏùºÎ∂Ä...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 4\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 27/33 [00:21<00:04,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "Êñ∞ÊôÇ‰ª£„ÅÆ„Ç¥„É´„Éï„É™„Éº„Ç∞„ÄåTGL„Äç„Åå„ÅÑ„Çà„ÅÑ„ÇàÈñãÂπïÔºÅ„É´„Éº„É´„ÅØÔºüÂ†¥ÊâÄ„ÅØÔºüÂá∫Â†¥ËÄÖ„ÅØÔºü„Åæ„Å®„ÇÅ„Å¶„ÉÅ„Çß„ÉÉ„ÇØÔºÅ„Äê„Ç¥„É´„Éï„Äë Êñ∞ÊôÇ‰ª£„Ç¥„É´„Éï„É™„Éº„Ç∞„ÉªTGL\n",
            "1Êúà8Êó•„ÅÆÈñãÂπïÊà¶„Äå„Éã„É•„Éº„É®„Éº„ÇØ vs. „Éô„Ç§„Äç„ÅÆË¶ñËÅ¥„ÅØ„Åì„Å°„Çâ„Åã„Çâ‚Üì\n",
            "https://video.unext.jp/livedetail/LIV0000007451...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 5\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 28/33 [00:21<00:03,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "ÌÉÄÏù¥Í±∞ Ïö∞Ï¶àÏùò TGLÏóê ÎßûÏÑ† Í≥®ÌîÑÏ°¥Ïùò ÎπÑÎ∞Ä Ï†ÑÎûµ! ÎìúÎîîÏñ¥ Í≥µÍ∞úÎêú ÏãúÌã∞Í≥®ÌîÑÏùò Ï£ºÏöî ÎÇ¥Ïö©ÏùÑ Ï†ïÎ¶¨Ìï¥Î¥§ÏäµÎãàÎã§. ÌÉÄÏù¥Í±∞ Ïö∞Ï¶àÏôÄ Î°úÎ¶¨ Îß•Í∏∏Î°úÏù¥Ïùò TGLÏ∂úÎ≤îÏùÑ ÏïûÎëêÍ≥† Í≥®ÌîÑÏ°¥Ïù¥ Ïù¥Ïóê ÎßûÏÑú Í∑∏ÎèôÏïà ÎπÑÎ∞ÄÎ¶¨Ïóê Ï§ÄÎπÑÌïú Ï†ÑÎûµÏù¥ ÎìúÎîîÏñ¥ Í≥µÍ∞úÎêòÏóàÏäµÎãàÎã§.\n",
            "ÏÉàÎ°úÏö¥ ÌòïÌÉúÏùò Í≥®ÌîÑÏ°¥Ïùò Ïä§ÌÅ¨Î¶∞ Í≥®ÌîÑÎäî Î¨¥ÏóáÏù¥Í≥†, TGLÍ≥ºÎäî Ïñ¥ÎñªÍ≤å Îã§Î•∏ÏßÄ ÍººÍººÌïòÍ≤å Ï†ïÎ¶¨Ìï¥Î¥§ÏäµÎãàÎã§. \n",
            "\n",
            "Time Stamp\n",
            "00:00...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 4\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 29/33 [00:22<00:02,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "ÁΩóÈáå¬∑È∫¶ÂÖãÁΩó‰ºäÂ•¢ÂçéÁîüÊ¥ªÊñπÂºè Rory Mcllroy Luxurious Lifestyle #golf #golfswing\n",
            "\n",
            "Rory McIlroy, the Irish professional golfer, has a net worth of $170 million. Rory McIlroy, one of the most successful golfers in the w...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 2\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 30/33 [00:23<00:02,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "Does Netflix Want in on Live Sports? - PGA Tour, Formula 1 in \"Netflix Cup\" | The First Cut Podcast #golf #pgatour #formula1 #netflixcup #netflix \n",
            "\n",
            "The PGA Tour and Formula 1 will be involved in one o...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 2\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 31/33 [00:23<00:01,  1.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "Jon Rahm's PLEA To Allow Rebels To Compete In The Tournament.. Jon Rahm's PLEA To Allow Rebels To Compete In The Tournament..\n",
            "\n",
            "Welcome back to Sport Shock. Bit by bit, day by day, the world‚Äôs best gol...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 2\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rClassifying Rows:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 32/33 [00:24<00:00,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "The Good Good Hickory Clubs Major Thanks to SoFi for sponsoring the video! Click here to sign-up for SoFi Checking and Savings!\n",
            "‚ñ∫ https://sofi.com/goodgood\n",
            "\n",
            "Our Apparel ‚ñ∫ https://goodgoodgolf.com/\n",
            "\n",
            "Th...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 3\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Classifying Rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:25<00:00,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document Preview:\n",
            "CURRENT AFFAIRS for BANKING EXAMS: 7th January, 2025 with SHOTS Stay updated with the most important Current Affairs for Banking Exams of 7th January, 2025  with SHOTS - Short Highlights of Top Storie...\n",
            "\n",
            "Validation Result:\n",
            "{\n",
            "  \"filter_rating\": 1\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification complete! Results saved to: /content/drive/MyDrive/DATA/TGL_classified.xlsx\n",
            "\n",
            "Rating Summary:\n",
            "filter_rating\n",
            "5    20\n",
            "4     5\n",
            "2     5\n",
            "1     2\n",
            "3     1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df1 = pd.read_excel(\"/content/drive/MyDrive/DATA/TGL_classified.xlsx\")\n",
        "\n",
        "# Select only the required columns\n",
        "df1 = df1[[\"title\", \"id_author\", \"description\", \"filter_rating\"]]\n",
        "\n",
        "# Adjust pandas display settings to show full text\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Sort ds1 by 'filter_rating' in descending order\n",
        "df1 = df1.sort_values(by=\"filter_rating\", ascending=False)\n",
        "\n",
        "# Display the filtered and sorted DataFrame\n",
        "df1"
      ],
      "metadata": {
        "id": "C3TembUfaGja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Major Takeaways\n",
        "\n",
        "- Use LLMs as intelligent assistants for scalable data analysis.\n",
        "\n",
        "- Clear prompts and tools like Google Colab and the OpenAI API simplify complex filtering tasks.\n",
        "\n",
        "- Structured outputs (e.g., JSON) and controlled parameters (temperature=0) ensure consistency and ease of integration.\n",
        "\n",
        "- Include manual classification options for cases where automated methods fall short.\n"
      ],
      "metadata": {
        "id": "v54nK1JNoi_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Scholar publication extraction & ABDC ranking analysis\n",
        "\n",
        "This script automates the extraction of publication details from Google Scholar content and evaluates them against the ABDC Journal Rankings. It processes scholarly content, extracts metadata, and performs ranking analysis to assess journal quality.\n",
        "\n",
        "### Key Features:\n",
        "- Extract publication titles, outlet names, and publication years from input text.\n",
        "- Use GPT-4o to structure extracted data into JSON format.\n",
        "- Compare extracted journals against the ABDC database to classify them by ranking (A\\*, A, B, C).\n",
        "- Load data from JSON and Excel files for systematic ranking analysis.\n",
        "- Output matched/unmatched journals, ranking distribution, and validation checks."
      ],
      "metadata": {
        "id": "8doyDctIbzLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths for Google Colab\n",
        "JSON_PATH = \"/content/drive/MyDrive/Teaching/Scholar/scholar_publications.json\"\n",
        "MARKDOWN_PATH = \"/content/drive/MyDrive/Teaching/Scholar/ABDC_Q.md\"\n",
        "\n",
        "class ScholarAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.scholar_content = None\n",
        "        self.json_result = None\n",
        "        self.publications = []\n",
        "\n",
        "    def get_user_input(self):\n",
        "        \"\"\"Display text area for Google Scholar content\"\"\"\n",
        "        text_area = widgets.Textarea(\n",
        "            placeholder='Paste Google Scholar content here...',\n",
        "            description='Scholar Content:',\n",
        "            layout={'width': '100%', 'height': '300px'}\n",
        "        )\n",
        "        button = widgets.Button(description=\"Process Scholar Content\")\n",
        "\n",
        "        display(text_area)\n",
        "        display(button)\n",
        "\n",
        "        def on_button_clicked(b):\n",
        "            self.scholar_content = text_area.value.strip()\n",
        "            if not self.scholar_content:\n",
        "                print(\"Error: No content provided\")\n",
        "                return\n",
        "\n",
        "            button.description = \"Processing...\"\n",
        "            button.disabled = True\n",
        "\n",
        "            # Process the content\n",
        "            self.process_scholar_content()\n",
        "\n",
        "        button.on_click(on_button_clicked)\n",
        "\n",
        "    def process_scholar_content(self):\n",
        "        \"\"\"Process Google Scholar content using OpenAI API\"\"\"\n",
        "        print(\"Processing Google Scholar content...\")\n",
        "\n",
        "        try:\n",
        "            client = OpenAI(api_key=api_key)\n",
        "\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"Extract publications information from a Google Scholar page content and output in JSON format. Each publication should include the outlet name, article title, and publication year. Additionally, calculate and include the total number of publications.\\n\\nOutput Format:\\n- JSON object with two fields:\\n  - \\\"Publications\\\": A JSON list of objects, each with three fields: \\\"Outlet_Name\\\", \\\"Article_Title\\\", and \\\"Publication_Year\\\".\\n  - \\\"Total_Publications\\\": A number representing the total count of publications.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": self.scholar_content\n",
        "                    }\n",
        "                ],\n",
        "                response_format={\"type\": \"text\"},\n",
        "                temperature=0,\n",
        "                max_tokens=16383\n",
        "            )\n",
        "\n",
        "            response_content = response.choices[0].message.content.strip()\n",
        "\n",
        "            # Clean the JSON response\n",
        "            if response_content.startswith(\"```json\"):\n",
        "                response_content = response_content[7:]\n",
        "            if response_content.endswith(\"```\"):\n",
        "                response_content = response_content[:-3]\n",
        "\n",
        "            response_content = response_content.strip()\n",
        "\n",
        "            try:\n",
        "                self.json_result = json.loads(response_content)\n",
        "                print(f\"Successfully extracted {self.json_result.get('Total_Publications', 0)} publications\")\n",
        "\n",
        "                # Display JSON preview\n",
        "                print(\"\\nJSON Preview:\")\n",
        "                display(JSON(self.json_result))\n",
        "\n",
        "                # Save the result\n",
        "                self.save_json_result()\n",
        "\n",
        "                # Continue to ABDC analysis\n",
        "                self.analyze_abdc_rankings()\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error parsing JSON: {str(e)}\")\n",
        "                print(\"Raw response:\\n\", response_content[:500] + \"...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing content: {str(e)}\")\n",
        "\n",
        "    def save_json_result(self):\n",
        "        \"\"\"Save JSON results to file\"\"\"\n",
        "        try:\n",
        "            # Ensure directory exists\n",
        "            os.makedirs(os.path.dirname(JSON_PATH), exist_ok=True)\n",
        "\n",
        "            with open(JSON_PATH, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.json_result, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            print(f\"Results saved to {JSON_PATH}\")\n",
        "\n",
        "            # Verify file exists\n",
        "            if os.path.exists(JSON_PATH):\n",
        "                print(\"File verification successful ‚úì\")\n",
        "            else:\n",
        "                print(\"Warning: File not found after saving!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving JSON: {str(e)}\")\n",
        "\n",
        "    def load_publications(self):\n",
        "        \"\"\"Load publications from JSON file\"\"\"\n",
        "        try:\n",
        "            with open(JSON_PATH, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                self.publications = data.get('Publications', [])\n",
        "                print(f\"Loaded {len(self.publications)} publications from file\")\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading publications: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def load_abdc_rankings(self):\n",
        "        \"\"\"Load ABDC rankings from Markdown file\"\"\"\n",
        "        try:\n",
        "            # Read the markdown file\n",
        "            with open(MARKDOWN_PATH, 'r', encoding='utf-8') as f:\n",
        "                markdown_content = f.read()\n",
        "\n",
        "            # Parse markdown table content\n",
        "            # Assuming the markdown file contains a table with pipe separators\n",
        "            lines = markdown_content.strip().split('\\n')\n",
        "\n",
        "            # Skip header and separator lines\n",
        "            data_lines = [line for line in lines if '|' in line]\n",
        "            if len(data_lines) > 2:  # Ensure we have header, separator, and at least one data row\n",
        "                data_lines = data_lines[2:]  # Skip header and separator rows\n",
        "\n",
        "            # Create dataframe\n",
        "            journal_data = []\n",
        "            for line in data_lines:\n",
        "                cells = [cell.strip() for cell in line.split('|')]\n",
        "                cells = [cell for cell in cells if cell]  # Remove empty cells from edges\n",
        "                if len(cells) >= 2:  # Ensure we have at least journal name and ranking\n",
        "                    journal_data.append({\n",
        "                        'Journal_Name': cells[0],\n",
        "                        'Ranking': cells[1]\n",
        "                    })\n",
        "\n",
        "            df = pd.DataFrame(journal_data)\n",
        "\n",
        "            # Clean and standardize journal names and rankings\n",
        "            df['Journal_Name'] = df['Journal_Name'].str.lower().str.strip()\n",
        "            df['Ranking'] = df['Ranking'].str.strip()\n",
        "\n",
        "            print(f\"Loaded {len(df)} ABDC journal entries from markdown file\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading ABDC rankings from markdown: {str(e)}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def save_analysis_to_markdown(self, results):\n",
        "        \"\"\"Save analysis results to markdown file\"\"\"\n",
        "        try:\n",
        "            output_path = \"/content/drive/MyDrive/Teaching/Scholar/Analysis_results.md\"\n",
        "\n",
        "            summary = results['Summary']\n",
        "            matched_journals = sorted(\n",
        "                results['Matched_Journals'],\n",
        "                key=lambda x: (x['Ranking'], -int(x['Year']) if str(x['Year']).isdigit() else 0)\n",
        "            )\n",
        "            unmatched_journals = sorted(\n",
        "                results['Unmatched_Journals'],\n",
        "                key=lambda x: -int(x['Year']) if str(x['Year']).isdigit() else 0\n",
        "            )\n",
        "\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                # Title\n",
        "                f.write(\"# ABDC Rankings Analysis\\n\\n\")\n",
        "\n",
        "                # Date\n",
        "                from datetime import datetime\n",
        "                current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                f.write(f\"*Analysis generated on: {current_date}*\\n\\n\")\n",
        "\n",
        "                # Summary section\n",
        "                f.write(\"## Summary\\n\\n\")\n",
        "                f.write(\"| Category | Count |\\n\")\n",
        "                f.write(\"|----------|------:|\\n\")\n",
        "                f.write(f\"| Total Publications | {summary['Total_Publications']} |\\n\")\n",
        "                f.write(f\"| ABDC Listed Publications | {summary['Total_ABDC']} |\\n\")\n",
        "                f.write(f\"| A* Publications | {summary['A*']} |\\n\")\n",
        "                f.write(f\"| A Publications | {summary['A']} |\\n\")\n",
        "                f.write(f\"| B Publications | {summary['B']} |\\n\")\n",
        "                f.write(f\"| C Publications | {summary['C']} |\\n\")\n",
        "                f.write(f\"| Non-ABDC Publications | {summary['Non_ABDC']} |\\n\\n\")\n",
        "\n",
        "                # Matched journals section\n",
        "                f.write(\"## ABDC Matched Publications\\n\\n\")\n",
        "\n",
        "                # Group by ranking\n",
        "                current_ranking = None\n",
        "                for journal in matched_journals:\n",
        "                    if current_ranking != journal['Ranking']:\n",
        "                        current_ranking = journal['Ranking']\n",
        "                        f.write(f\"### {current_ranking} Ranked Publications\\n\\n\")\n",
        "\n",
        "                    f.write(f\"**{journal['Year']} - {journal['Journal']}**\\n\\n\")\n",
        "                    f.write(f\"{journal['Title']}\\n\\n\")\n",
        "\n",
        "                # Unmatched journals section\n",
        "                f.write(\"## Non-ABDC Publications\\n\\n\")\n",
        "                for journal in unmatched_journals:\n",
        "                    f.write(f\"**{journal['Year']} - {journal['Journal']}**\\n\\n\")\n",
        "                    f.write(f\"{journal['Title']}\\n\\n\")\n",
        "\n",
        "            print(f\"\\nAnalysis saved to markdown file: {output_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving analysis to markdown: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def analyze_abdc_rankings(self):\n",
        "        \"\"\"Analyze publications against ABDC rankings\"\"\"\n",
        "        print(\"\\nAnalyzing publications against ABDC rankings...\")\n",
        "\n",
        "        # If we're coming from JSON processing, use the publications from there\n",
        "        if not self.publications and self.json_result:\n",
        "            self.publications = self.json_result.get('Publications', [])\n",
        "\n",
        "        # Otherwise try to load from file\n",
        "        if not self.publications:\n",
        "            if not self.load_publications():\n",
        "                print(\"No publications to analyze. Please process Google Scholar content first.\")\n",
        "                return\n",
        "\n",
        "        # Load ABDC rankings\n",
        "        rankings_df = self.load_abdc_rankings()\n",
        "        if rankings_df.empty:\n",
        "            print(\"Failed to load ABDC rankings from markdown file\")\n",
        "            return\n",
        "\n",
        "        # Create results structure\n",
        "        results = {\n",
        "            'Total_Publications': len(self.publications),\n",
        "            'Rankings': defaultdict(int, {\n",
        "                'A*': 0, 'A': 0, 'B': 0, 'C': 0, 'Non-ABDC': 0\n",
        "            }),\n",
        "            'Matched_Journals': [],\n",
        "            'Unmatched_Journals': []\n",
        "        }\n",
        "\n",
        "        # Create lookup dictionary for faster matching\n",
        "        rankings_lookup = dict(zip(rankings_df['Journal_Name'], rankings_df['Ranking']))\n",
        "\n",
        "        # Match publications with rankings\n",
        "        for pub in self.publications:\n",
        "            matched, ranking = self.find_journal_match(pub['Outlet_Name'], rankings_lookup)\n",
        "\n",
        "            if matched:\n",
        "                # Count the ranking\n",
        "                results['Rankings'][ranking] += 1\n",
        "\n",
        "                # Add to matched journals list\n",
        "                results['Matched_Journals'].append({\n",
        "                    'Journal': pub['Outlet_Name'],\n",
        "                    'Ranking': ranking,\n",
        "                    'Year': pub['Publication_Year'],\n",
        "                    'Title': pub['Article_Title']\n",
        "                })\n",
        "            else:\n",
        "                results['Rankings']['Non-ABDC'] += 1\n",
        "                results['Unmatched_Journals'].append({\n",
        "                    'Journal': pub['Outlet_Name'],\n",
        "                    'Year': pub['Publication_Year'],\n",
        "                    'Title': pub['Article_Title']\n",
        "                })\n",
        "\n",
        "        # Calculate summary statistics\n",
        "        total_abdc = (results['Rankings']['A*'] +\n",
        "                      results['Rankings']['A'] +\n",
        "                      results['Rankings']['B'] +\n",
        "                      results['Rankings']['C'])\n",
        "\n",
        "        results['Summary'] = {\n",
        "            'Total_Publications': results['Total_Publications'],\n",
        "            'Total_ABDC': total_abdc,\n",
        "            'A*': results['Rankings']['A*'],\n",
        "            'A': results['Rankings']['A'],\n",
        "            'B': results['Rankings']['B'],\n",
        "            'C': results['Rankings']['C'],\n",
        "            'Non_ABDC': results['Rankings']['Non-ABDC']\n",
        "        }\n",
        "\n",
        "        # Print the analysis report\n",
        "        self.print_analysis_report(results)\n",
        "\n",
        "    def find_journal_match(self, journal_name, rankings_lookup):\n",
        "        \"\"\"Find if a journal matches any in the ABDC list with 100% match requirement\"\"\"\n",
        "        journal_name_lower = journal_name.lower().strip()\n",
        "\n",
        "        # Try exact match only (case-insensitive)\n",
        "        if journal_name_lower in rankings_lookup:\n",
        "            ranking = rankings_lookup[journal_name_lower].strip()\n",
        "            return True, ranking\n",
        "\n",
        "        # No partial matching or special cases anymore\n",
        "        return False, None\n",
        "\n",
        "    def print_analysis_report(self, results):\n",
        "        \"\"\"Print detailed analysis report and save to markdown file\"\"\"\n",
        "        summary = results['Summary']\n",
        "\n",
        "        # Print to console\n",
        "        print(\"\\nABDC Rankings Analysis:\")\n",
        "        print(\"-----------------------\")\n",
        "        print(f\"Total Publications: {summary['Total_Publications']}\")\n",
        "        print(f\"ABDC Listed Publications: {summary['Total_ABDC']}\")\n",
        "        print(f\"A* Publications: {summary['A*']}\")\n",
        "        print(f\"A Publications: {summary['A']}\")\n",
        "        print(f\"B Publications: {summary['B']}\")\n",
        "        print(f\"C Publications: {summary['C']}\")\n",
        "        print(f\"Non-ABDC Publications: {summary['Non_ABDC']}\")\n",
        "\n",
        "        print(\"\\nMatched ABDC Journals:\")\n",
        "        print(\"----------------------\")\n",
        "        # Sort by ranking and year\n",
        "        matched_journals = sorted(\n",
        "            results['Matched_Journals'],\n",
        "            key=lambda x: (x['Ranking'], -int(x['Year']) if str(x['Year']).isdigit() else 0)\n",
        "        )\n",
        "\n",
        "        # Group by ranking\n",
        "        current_ranking = None\n",
        "        for journal in matched_journals:\n",
        "            if current_ranking != journal['Ranking']:\n",
        "                current_ranking = journal['Ranking']\n",
        "                print(f\"\\n{current_ranking} Ranked Publications:\")\n",
        "                print(\"-\" * (len(current_ranking) + 20))\n",
        "\n",
        "            print(f\"{journal['Year']} - {journal['Journal']}\")\n",
        "            print(f\"Title: {journal['Title']}\")\n",
        "\n",
        "        print(\"\\nUnmatched Journals:\")\n",
        "        print(\"-----------------\")\n",
        "        unmatched_journals = sorted(\n",
        "            results['Unmatched_Journals'],\n",
        "            key=lambda x: -int(x['Year']) if str(x['Year']).isdigit() else 0\n",
        "        )\n",
        "        for journal in unmatched_journals:\n",
        "            print(f\"{journal['Year']} - {journal['Journal']}\")\n",
        "            print(f\"Title: {journal['Title']}\")\n",
        "\n",
        "        print(\"\\nAnalysis complete!\")\n",
        "\n",
        "        # Save to markdown file\n",
        "        self.save_analysis_to_markdown(results)\n",
        "\n",
        "# Run the analyzer\n",
        "analyzer = ScholarAnalyzer()\n",
        "\n",
        "# Option to run just the ABDC analysis (if JSON file already exists)\n",
        "run_only_analysis = False  # Set to True to skip Scholar content extraction\n",
        "\n",
        "if run_only_analysis:\n",
        "    analyzer.analyze_abdc_rankings()\n",
        "else:\n",
        "    analyzer.get_user_input()"
      ],
      "metadata": {
        "id": "HthrCBmbVWuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next: Develop a custom multi-agent system for your work"
      ],
      "metadata": {
        "id": "9MOSxqscxvWI"
      }
    }
  ]
}